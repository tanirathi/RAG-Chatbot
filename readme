"""ğŸ“Œ Basic Explanation of the RAG Model in This Implementation
This Retrieval-Augmented Generation (RAG) model is designed to process dialysis patient data from a CSV file and allow users to query the data using a chatbot interface.

ğŸ”¹ How It Works
1ï¸âƒ£ Data Loading: The system reads and processes the CSV file using pandas.
2ï¸âƒ£ Vector Database Creation (FAISS): The text data is converted into embeddings and stored in a FAISS index for fast retrieval.
3ï¸âƒ£ Query Handling: When a user asks a question, the model retrieves relevant documents from FAISS.
4ï¸âƒ£ Response Generation (LLM): The retrieved information is passed to an LLM (Large Language Model) to generate a natural language answer.
5ï¸âƒ£ Chatbot UI (Streamlit): A user-friendly chatbot interface is built using Streamlit, allowing conversational interaction with the dataset.

ğŸ“Œ Technologies & Models Used
Below are the key technologies and models used in the implementation:

1ï¸âƒ£ FAISS (Facebook AI Similarity Search)
ğŸ“Œ What is FAISS?

FAISS is an efficient vector database used for fast retrieval of similar text chunks.
It allows quick similarity searches in large datasets.
ğŸ“Œ Why is FAISS used here?

Instead of searching raw text in a CSV file, we convert text into vectors and store them in FAISS, making queries much faster.
ğŸ“Œ How FAISS works in this project?

Step 1: The CSV data is converted into text chunks.
Step 2: These chunks are embedded using a sentence transformer (all-MiniLM-L6-v2).
Step 3: FAISS stores and retrieves the most relevant chunks based on similarity.
2ï¸âƒ£ SentenceTransformer (all-MiniLM-L6-v2)
ğŸ“Œ What is this model?

all-MiniLM-L6-v2 is a sentence embedding model from Hugging Face.
It converts text into vector representations, which helps in similarity searches.
ğŸ“Œ Why are embeddings needed?

To compare text effectively, we need a numerical representation.
Example: If a user asks, "What is the body temperature of Patient 2?", we find similar text chunks instead of doing an exact word match.
ğŸ“Œ Why use all-MiniLM-L6-v2 instead of other models?

Faster and lightweight âœ…
Good accuracy for sentence similarity âœ…
Works well with FAISS âœ…
3ï¸âƒ£ LLM (Large Language Model) â€“ Moondream
ğŸ“Œ What is an LLM?

An LLM is an AI model trained to generate human-like text.
It takes retrieved text and generates a meaningful answer.
ğŸ“Œ Why use moondream LLM?

Itâ€™s optimized for answering queries based on context.
It integrates well with FAISS and Streamlit.
ğŸ“Œ How does the LLM work here?

Retrieves relevant documents from FAISS.
Uses the retrieved context to generate a natural response.
Returns the final answer to the user.
4ï¸âƒ£ Streamlit (For Chatbot UI)
ğŸ“Œ Why use Streamlit?

Itâ€™s a lightweight framework for building interactive web apps.
Perfect for AI and ML models that require a simple chatbot-like UI.
ğŸ“Œ What does Streamlit do in this project?

File Upload: Allows users to upload a CSV file.
Chat Interface: Displays user queries & responses.
Real-time Processing: Shows retrieved documents and their sources.
ğŸ“Œ Overall Flow of the Model
1ï¸âƒ£ User uploads a CSV file â†’ The data is loaded.
2ï¸âƒ£ FAISS creates a vector store â†’ Data is split into chunks and embedded.
3ï¸âƒ£ User enters a query â†’ FAISS retrieves the most relevant text.
4ï¸âƒ£ LLM generates an answer â†’ The retrieved context is used to generate a response.
5ï¸âƒ£ Chatbot displays the answer â†’ The result is shown to the user.

ğŸ”¹ Why is This Model Powerful?
âœ… Faster than traditional text searches (Uses FAISS for rapid retrieval).
âœ… Generates human-like responses (Uses an LLM instead of keyword matching).
âœ… Works on any dataset (CSV file can contain any structured data).
âœ… User-friendly UI (Built using Streamlit for easy interaction).

This RAG-based chatbot allows fast and interactive querying of patient data, making it ideal for medical data analysis, business intelligence, or any structured document search. ğŸš€

#implementation

import os
import streamlit as st
import pandas as pd
from langchain.document_loaders import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# FAISS index path
FAISS_INDEX_PATH = "faiss_index"

# 1ï¸âƒ£ **Load & Process Data Efficiently**
@st.cache_resource
def create_vector_db(data_path):  
    """Loads CSV, formats data into structured text, and creates FAISS vectorstore."""
    
    df = pd.read_csv(data_path)

    # âœ… Convert dataset into structured text (AI understands this better)
    records = df.to_dict(orient="records")
    documents = [
        f"Patient {rec['Patient_ID']} ({rec['Sex']}, {rec['Age']} years old)\n"
        f"- Date: {rec['Datetime']}\n"
        f"- Hypertension: {'Yes' if rec['Is_Hypertensive'] else 'No'}\n"
        f"- Diabetes: {'Yes' if rec['Is_Diabetic'] else 'No'}\n"
        f"- Dialyzer: {rec['Dialyzer']}, Technique: {rec['Type_of_Technique']}\n"
        f"- Body Temperature: {rec['Body_Temperature']}Â°C, Heart Rate: {rec['Heart_Rate']} bpm\n"
        f"- Blood Pressure: {rec['Systolic_BP']}/{rec['Diastolic_BP']} mmHg\n"
        f"- Urea Clearance: {rec['Urea_Clearance']}, Volume Changes: {rec['Volume_Changes']}\n"
        "------------------------------------------"
        for rec in records
    ]
    
    # âœ… Ensure text is split properly for FAISS
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.create_documents(documents)

    # âœ… Use Efficient Embedding Model
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

    # âœ… Load or Create FAISS Index
    if os.path.exists(FAISS_INDEX_PATH):  
        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
    else:
        db = FAISS.from_documents(chunks, embeddings)
        db.save_local(FAISS_INDEX_PATH)  
    
    return db

# 2ï¸âƒ£ **Setup LLM & Prompt for Retrieval-Based QA**
ollama_llm = OllamaLLM(base_url="http://localhost:11434", model="moondream")

# âœ… **Improved Prompt for Better AI Answers**
PROMPT_TEMPLATE = PromptTemplate(
    input_variables=["context", "question"],
    template="""You are an AI trained to analyze dialysis patient records.
    
    Given the following patient data:
    {context}
    
    Extract the precise answer from the dataset.
    
    Question: {question}
    
    If the answer is not found, respond with: "I could not find that information in the dataset."
    """
)

# 3ï¸âƒ£ **Streamlit UI: Chatbot**
st.set_page_config(page_title="Dialysis Data Chatbot", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ’¬ Dialysis Data Chatbot")

st.sidebar.title("âš™ï¸ Upload Data")
uploaded_file = st.sidebar.file_uploader("ğŸ“‚ Upload CSV File", type="csv")

if uploaded_file:
    db = create_vector_db(uploaded_file.name)
    retriever = db.as_retriever()

    # ğŸ”¹ **Chat Interface**
    st.subheader("ğŸ’¡ Chat with the Data")
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    input_txt = st.text_input("ğŸ§ Ask something about the data...")

    if st.button("ğŸ” Get Answer"):
        if input_txt:
            with st.spinner("ğŸ” Searching..."):
                retrieved_docs = retriever.get_relevant_documents(input_txt)

                if not retrieved_docs:
                    response = "âš ï¸ I could not find that information in the dataset."
                else:
                    # âœ… **Pass structured context to the AI**
                    context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # âœ… **Use Improved Prompt to Ensure Correct Answer Extraction**
                    qa_chain = RetrievalQA.from_chain_type(
                        llm=ollama_llm,
                        retriever=retriever,
                        chain_type="stuff",
                        return_source_documents=True,
                        chain_type_kwargs={"prompt": PROMPT_TEMPLATE}
                    )
                    result = qa_chain({"query": input_txt, "context": context_text})
                    response = result['result']

                # ğŸ”¹ **Update Chat History**
                st.session_state.chat_history.append(("User", input_txt))
                st.session_state.chat_history.append(("AI", response))

                # ğŸ”¹ **Display Chat**
                for sender, msg in st.session_state.chat_history:
                    if sender == "User":
                        st.markdown(f"**ğŸ§‘â€ğŸ’» {sender}:** {msg}")
                    else:
                        st.markdown(f"**ğŸ¤– {sender}:** {msg}")

else:
    st.info("ğŸ“‚ Please upload a CSV file to start.")"""